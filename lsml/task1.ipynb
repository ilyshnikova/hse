{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check for common deployment errors first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This should show three nodes with 500 - 1000 GB each (951 in this case). Ambari -> 'Start All' should be done first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS Remaining: 3037451872768 (2.76 TB)\n",
      "DFS Remaining: 1012518645248 (942.98 GB)\n",
      "DFS Remaining%: 94.00%\n",
      "DFS Remaining: 1011719310848 (942.24 GB)\n",
      "DFS Remaining%: 93.92%\n",
      "DFS Remaining: 1013213916672 (943.63 GB)\n",
      "DFS Remaining%: 94.06%\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -report | grep \"DFS Remaining\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spark_setup\n",
    "spark_setup.setup_pyspark_env()\n",
    "import spark_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambari - http://10.0.1.21:8080\n",
      "All Applications - http://10.0.1.23:8088/cluster\n",
      "CPU times: user 20 ms, sys: 8 ms, total: 28 ms\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc = spark_utils.get_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "hdfs_client = InsecureClient(\"http://cluster1:50070\", user='hdfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Download task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# place yours here\n",
    "student_id = 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your region is: westeurope\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "student_region = json.loads(open(\"../azure/regions.json\").read())[\"student{}\".format(student_id)]\n",
    "print \"your region is:\", student_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lsml1westeurope.blob.core.windows.net/data/task1.zip\n"
     ]
    }
   ],
   "source": [
    "task_data_link = \"https://lsml1{}.blob.core.windows.net/data/task1.zip\".format(student_region)\n",
    "print task_data_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92 ms, sys: 12 ms, total: 104 ms\n",
      "Wall time: 11min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# you can see progress in tmux (tab with running jupyter notebook)\n",
    "import os\n",
    "os.system(\"wget {} -O /data/task1.zip\".format(task_data_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 64 ms, total: 176 ms\n",
      "Wall time: 18min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# unzip task1.zip (many zip files inside)\n",
    "os.system(\"unzip /data/task1.zip -d /data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136M\t/data/clicks_test.csv.zip\r\n",
      "390M\t/data/clicks_train.csv.zip\r\n",
      "33M\t/data/documents_categories.csv.zip\r\n",
      "126M\t/data/documents_entities.csv.zip\r\n",
      "16M\t/data/documents_meta.csv.zip\r\n",
      "121M\t/data/documents_topics.csv.zip\r\n",
      "478M\t/data/events.csv.zip\r\n",
      "30G\t/data/page_views.csv.zip\r\n",
      "149M\t/data/page_views_sample.csv.zip\r\n",
      "2.6M\t/data/promoted_content.csv.zip\r\n",
      "100M\t/data/sample_submission.csv.zip\r\n",
      "32G\t/data/task1.zip\r\n"
     ]
    }
   ],
   "source": [
    "# verify that you're all set\n",
    "! du -sh /data/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://www.kaggle.com/c/outbrain-click-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        print '%r (%r, %r) %2.2f sec' % \\\n",
    "              (method.__name__, args, kw, te-ts)\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.delete(\"/task1\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'unzip_to_hdfs' (('clicks_test.csv.zip',), {}) 9.13 sec\n",
      "\n",
      "'unzip_to_hdfs' (('clicks_train.csv.zip',), {}) 19.54 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_categories.csv.zip',), {}) 3.58 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_entities.csv.zip',), {}) 14.93 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_meta.csv.zip',), {}) 3.01 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_topics.csv.zip',), {}) 7.02 sec\n",
      "\n",
      "'unzip_to_hdfs' (('events.csv.zip',), {}) 21.23 sec\n",
      "\n",
      "'unzip_to_hdfs' (('page_views.csv.zip',), {}) 1604.36 sec\n",
      "\n",
      "'unzip_to_hdfs' (('page_views_sample.csv.zip',), {}) 8.96 sec\n",
      "\n",
      "'unzip_to_hdfs' (('promoted_content.csv.zip',), {}) 2.36 sec\n",
      "\n",
      "'unzip_to_hdfs' (('sample_submission.csv.zip',), {}) 6.11 sec\n",
      "CPU times: user 204 ms, sys: 112 ms, total: 316 ms\n",
      "Wall time: 28min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "\n",
    "@timeit\n",
    "def unzip_to_hdfs(fn):\n",
    "    fn_out = fn.replace(\".zip\", \"\")\n",
    "    print subprocess.check_output(\"unzip -p /data/{0} | hadoop fs -put - /task1/{1}\".format(fn, fn_out), shell=True)\n",
    "    \n",
    "fns = [\n",
    "    \"clicks_test.csv.zip\",\n",
    "    \"clicks_train.csv.zip\",\n",
    "    \"documents_categories.csv.zip\",\n",
    "    \"documents_entities.csv.zip\",\n",
    "    \"documents_meta.csv.zip\",\n",
    "    \"documents_topics.csv.zip\",\n",
    "    \"events.csv.zip\",\n",
    "    \"page_views.csv.zip\",\n",
    "    \"page_views_sample.csv.zip\",\n",
    "    \"promoted_content.csv.zip\",\n",
    "    \"sample_submission.csv.zip\"\n",
    "]\n",
    "\n",
    "for fn in fns:\n",
    "    unzip_to_hdfs(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483.5 M  /task1/clicks_test.csv\n",
      "1.4 G  /task1/clicks_train.csv\n",
      "112.5 M  /task1/documents_categories.csv\n",
      "309.1 M  /task1/documents_entities.csv\n",
      "85.2 M  /task1/documents_meta.csv\n",
      "323.7 M  /task1/documents_topics.csv\n",
      "1.1 G  /task1/events.csv\n",
      "88.4 G  /task1/page_views.csv\n",
      "433.3 M  /task1/page_views_sample.csv\n",
      "13.2 M  /task1/promoted_content.csv\n",
      "260.5 M  /task1/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# files are written on cluster1 node only, need to balance HDFS on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancer bandwidth is set to 1000000000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -setBalancerBandwidth 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 732 ms, total: 2.82 s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hdfs balancer -threshold 5 > balancer.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pvdf = ss.read.csv(\"/task1/page_views.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uuid', 'string'),\n",
       " ('document_id', 'string'),\n",
       " ('timestamp', 'string'),\n",
       " ('platform', 'string'),\n",
       " ('geo_location', 'string'),\n",
       " ('traffic_source', 'string')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 32 ms, total: 88 ms\n",
      "Wall time: 8min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2034275448"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pvdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Parquet is faster than CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 136 ms, sys: 28 ms, total: 164 ms\n",
      "Wall time: 14min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pvdf.write.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.3 G  /task1/page_views.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/page_views.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pvdf2 = ss.read.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo_location=u'ES>07', count=139257),\n",
       " Row(geo_location=u'US>MT>756', count=676540),\n",
       " Row(geo_location=u'LT', count=145441),\n",
       " Row(geo_location=u'IL>01', count=21174),\n",
       " Row(geo_location=u'DZ', count=141209)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import display\n",
    "boo = pvdf2.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo_location=u'US>MS>673', count=849299),\n",
       " Row(geo_location=u'DZ', count=141209),\n",
       " Row(geo_location=u'US>MT>756', count=676540),\n",
       " Row(geo_location=u'US>NY', count=420207),\n",
       " Row(geo_location=u'CO>02', count=274301)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84 ms, sys: 16 ms, total: 100 ms\n",
      "Wall time: 8min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "boo = pvdf.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convert all to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test.parquet done\n",
      "clicks_train.parquet done\n",
      "documents_categories.parquet done\n",
      "documents_entities.parquet done\n",
      "documents_meta.parquet done\n",
      "documents_topics.parquet done\n",
      "events.parquet done\n",
      "page_views.parquet done\n",
      "page_views_sample.parquet done\n",
      "promoted_content.parquet done\n",
      "sample_submission.parquet done\n",
      "CPU times: user 84 ms, sys: 40 ms, total: 124 ms\n",
      "Wall time: 6min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_all_to_parquet():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".csv\"):\n",
    "            fn_after = fn.replace(\".csv\", \".parquet\")\n",
    "            path_before = task_dir + fn\n",
    "            path_after = task_dir + fn_after\n",
    "            if fn_after not in all_files:\n",
    "                # generate parquet\n",
    "                df = ss.read.csv(path_before, header=True)\n",
    "                df.write.parquet(path_after)\n",
    "            # remove csv, we have parquet now\n",
    "            hdfs_client.delete(path_before)\n",
    "            print fn_after, \"done\"\n",
    "\n",
    "convert_all_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.2 M  /task1/clicks_test.parquet\r\n",
      "367.5 M  /task1/clicks_train.parquet\r\n",
      "36.5 M  /task1/documents_categories.parquet\r\n",
      "184.0 M  /task1/documents_entities.parquet\r\n",
      "21.2 M  /task1/documents_meta.parquet\r\n",
      "183.3 M  /task1/documents_topics.parquet\r\n",
      "669.3 M  /task1/events.parquet\r\n",
      "47.3 G  /task1/page_views.parquet\r\n",
      "236.9 M  /task1/page_views_sample.parquet\r\n",
      "5.0 M  /task1/promoted_content.parquet\r\n",
      "184.2 M  /task1/sample_submission.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preview all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### /task1/clicks_test.parquet ###############\n",
      "+----------+------+\n",
      "|display_id| ad_id|\n",
      "+----------+------+\n",
      "|  17805143|288388|\n",
      "+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/clicks_train.parquet ###############\n",
      "+----------+-----+-------+\n",
      "|display_id|ad_id|clicked|\n",
      "+----------+-----+-------+\n",
      "|         1|42337|      0|\n",
      "+----------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_categories.parquet ###############\n",
      "+-----------+-----------+----------------+\n",
      "|document_id|category_id|confidence_level|\n",
      "+-----------+-----------+----------------+\n",
      "|    1544588|       1513|     0.263546236|\n",
      "+-----------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_entities.parquet ###############\n",
      "+-----------+--------------------+-----------------+\n",
      "|document_id|           entity_id| confidence_level|\n",
      "+-----------+--------------------+-----------------+\n",
      "|    1539011|e01ed0c4a3e8f8f35...|0.327269624728567|\n",
      "+-----------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_meta.parquet ###############\n",
      "+-----------+---------+------------+-------------------+\n",
      "|document_id|source_id|publisher_id|       publish_time|\n",
      "+-----------+---------+------------+-------------------+\n",
      "|     325048|      822|         253|2013-02-27 00:00:00|\n",
      "+-----------+---------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_topics.parquet ###############\n",
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|     801028|     280|0.0148711250868194|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/events.parquet ###############\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|68fb8eb72c49c4|    1201414| 63621328|       3|       GB>F8|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views_sample.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|7504d9623fdc7e|        234| 72194818|       1|   US>CA>825|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/promoted_content.parquet ###############\n",
      "+-----+-----------+-----------+-------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|\n",
      "+-----+-----------+-----------+-------------+\n",
      "|    1|       6614|          1|            7|\n",
      "+-----+-----------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/sample_submission.parquet ###############\n",
      "+----------+--------------------+\n",
      "|display_id|               ad_id|\n",
      "+----------+--------------------+\n",
      "|  21960532|50582 190398 2293...|\n",
      "+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 32 ms, sys: 4 ms, total: 36 ms\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preview_all_files():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        df = ss.read.parquet(task_dir + fn)\n",
    "        print \"#\" * 15 + \" {0} \".format(task_dir + fn) + \"#\" * 15\n",
    "        df.show(1)\n",
    "        \n",
    "preview_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Register all tables to be usable in SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test done\n",
      "clicks_train done\n",
      "documents_categories done\n",
      "documents_entities done\n",
      "documents_meta done\n",
      "documents_topics done\n",
      "events done\n",
      "page_views done\n",
      "page_views_sample done\n",
      "promoted_content done\n",
      "sample_submission done\n",
      "CPU times: user 28 ms, sys: 4 ms, total: 32 ms\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def register_all_tables():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".parquet\"):\n",
    "            table_name = fn.replace(\".parquet\", \"\")\n",
    "            df = ss.read.parquet(task_dir + fn)\n",
    "            df.registerTempTable(table_name)\n",
    "            print table_name, \"done\"\n",
    "        \n",
    "register_all_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SQL query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(users_count=19794967)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ss.sql(\"\"\"\n",
    "select count(distinct(uuid)) as users_count\n",
    "from events\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Simple model using the following features:\n",
    "- **clicked**\n",
    "- geo_location features (country, state, dma)\n",
    "- day_of_week (from timestamp, use *date.isoweekday()*)\n",
    "- ad_id\n",
    "- ad_document_id\n",
    "- campaign_id\n",
    "- advertiser_id\n",
    "- display_document_id\n",
    "- platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(database=u'', tableName=u'clicks_test', isTemporary=True),\n",
       " Row(database=u'', tableName=u'clicks_train', isTemporary=True),\n",
       " Row(database=u'', tableName=u'documents_categories', isTemporary=True),\n",
       " Row(database=u'', tableName=u'documents_entities', isTemporary=True),\n",
       " Row(database=u'', tableName=u'documents_meta', isTemporary=True),\n",
       " Row(database=u'', tableName=u'documents_topics', isTemporary=True),\n",
       " Row(database=u'', tableName=u'events', isTemporary=True),\n",
       " Row(database=u'', tableName=u'geo', isTemporary=True),\n",
       " Row(database=u'', tableName=u'merged', isTemporary=True),\n",
       " Row(database=u'', tableName=u'page_views', isTemporary=True),\n",
       " Row(database=u'', tableName=u'page_views_sample', isTemporary=True),\n",
       " Row(database=u'', tableName=u'promoted_content', isTemporary=True),\n",
       " Row(database=u'', tableName=u'sample_submission', isTemporary=True)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "show tables\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test: [u'display_id', u'ad_id']\n",
      "clicks_train: [u'display_id', u'ad_id', u'clicked']\n",
      "documents_categories: [u'document_id', u'category_id', u'confidence_level']\n",
      "documents_entities: [u'document_id', u'entity_id', u'confidence_level']\n",
      "documents_meta: [u'document_id', u'source_id', u'publisher_id', u'publish_time']\n",
      "documents_topics: [u'document_id', u'topic_id', u'confidence_level']\n",
      "events: [u'display_id', u'uuid', u'document_id', u'timestamp', u'platform', u'geo_location']\n",
      "page_views: [u'uuid', u'document_id', u'timestamp', u'platform', u'geo_location', u'traffic_source']\n",
      "page_views_sample: [u'uuid', u'document_id', u'timestamp', u'platform', u'geo_location', u'traffic_source']\n",
      "promoted_content: [u'ad_id', u'document_id', u'campaign_id', u'advertiser_id']\n",
      "sample_submission: [u'display_id', u'ad_id']\n"
     ]
    }
   ],
   "source": [
    "field_to_table = {}\n",
    "\n",
    "task_dir = \"/task1/\"\n",
    "all_files = hdfs_client.list(task_dir)\n",
    "for table_name in all_files:\n",
    "    if \".parquet\" in table_name:\n",
    "        table_name = table_name.replace(\".parquet\", \"\")\n",
    "        query = \"DESCRIBE %s\" % table_name\n",
    "        #print(query)\n",
    "        res = ss.sql(query)\n",
    "        for i in res.collect():\n",
    "            if i['col_name'] in field_to_table:\n",
    "                field_to_table[i['col_name']] += [table_name]\n",
    "            else:\n",
    "                field_to_table[i['col_name']] = [table_name]\n",
    "        fields = [i['col_name'] for i in res.collect()]\n",
    "        print('%s: %s' % (table_name, str(fields)))\n",
    "        #print(merged.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked : [u'clicks_train']\n",
      "geo_location : [u'events', u'page_views', u'page_views_sample']\n",
      "timestamp : [u'events', u'page_views', u'page_views_sample']\n",
      "ad_id : [u'clicks_test', u'clicks_train', u'promoted_content', u'sample_submission']\n",
      "document_id : [u'documents_categories', u'documents_entities', u'documents_meta', u'documents_topics', u'events', u'page_views', u'page_views_sample', u'promoted_content']\n",
      "campaign_id : [u'promoted_content']\n",
      "advertiser_id : [u'promoted_content']\n",
      "platform : [u'events', u'page_views', u'page_views_sample']\n"
     ]
    }
   ],
   "source": [
    "['display_document_id', 'ad_document_id']\n",
    "\n",
    "fields = [\n",
    "    'clicked',\n",
    "    'geo_location',\n",
    "    'timestamp',\n",
    "    'ad_id',\n",
    "    'document_id',\n",
    "    'campaign_id',\n",
    "    'advertiser_id',\n",
    "    'platform'\n",
    "]\n",
    "for i in fields:\n",
    "    print(i + \" : \" + str(field_to_table[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'ad_id': [u'clicks_test',\n",
       "  u'clicks_train',\n",
       "  u'promoted_content',\n",
       "  u'sample_submission'],\n",
       " u'advertiser_id': [u'promoted_content'],\n",
       " u'campaign_id': [u'promoted_content'],\n",
       " u'category_id': [u'documents_categories'],\n",
       " u'clicked': [u'clicks_train'],\n",
       " u'confidence_level': [u'documents_categories',\n",
       "  u'documents_entities',\n",
       "  u'documents_topics'],\n",
       " u'display_id': [u'clicks_test',\n",
       "  u'clicks_train',\n",
       "  u'events',\n",
       "  u'sample_submission'],\n",
       " u'document_id': [u'documents_categories',\n",
       "  u'documents_entities',\n",
       "  u'documents_meta',\n",
       "  u'documents_topics',\n",
       "  u'events',\n",
       "  u'page_views',\n",
       "  u'page_views_sample',\n",
       "  u'promoted_content'],\n",
       " u'entity_id': [u'documents_entities'],\n",
       " u'geo_location': [u'events', u'page_views', u'page_views_sample'],\n",
       " u'platform': [u'events', u'page_views', u'page_views_sample'],\n",
       " u'publish_time': [u'documents_meta'],\n",
       " u'publisher_id': [u'documents_meta'],\n",
       " u'source_id': [u'documents_meta'],\n",
       " u'timestamp': [u'events', u'page_views', u'page_views_sample'],\n",
       " u'topic_id': [u'documents_topics'],\n",
       " u'traffic_source': [u'page_views', u'page_views_sample'],\n",
       " u'uuid': [u'events', u'page_views', u'page_views_sample']}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_to_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = ss.sql('''\n",
    "    select \n",
    "        clicked,\n",
    "        clicks_train.ad_id,\n",
    "        document_id, \n",
    "        campaign_id,\n",
    "        advertiser_id\n",
    "    from \n",
    "        clicks_train\n",
    "    join \n",
    "        promoted_content \n",
    "    on \n",
    "        clicks_train.ad_id = promoted_content.ad_id\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged.registerTempTable(\"merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo_location : [u'events', u'page_views', u'page_views_sample']\n",
      "timestamp : [u'events', u'page_views', u'page_views_sample']\n",
      "document_id : [u'documents_categories', u'documents_entities', u'documents_meta', u'documents_topics', u'events', u'page_views', u'page_views_sample', u'promoted_content']\n",
      "platform : [u'events', u'page_views', u'page_views_sample']\n",
      "display_id : [u'clicks_test', u'clicks_train', u'events', u'sample_submission']\n"
     ]
    }
   ],
   "source": [
    "['display_document_id', 'ad_document_id']\n",
    "\n",
    "fields = [\n",
    "    'geo_location',\n",
    "    'timestamp',\n",
    "    'document_id',\n",
    "    'platform',\n",
    "    'display_id'\n",
    "]\n",
    "for i in fields:\n",
    "    print(i + \" : \" + str(field_to_table[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = ss.sql('''\n",
    "    select \n",
    "        merged.clicked,\n",
    "        merged.ad_id,\n",
    "        merged.document_id, \n",
    "        merged.campaign_id,\n",
    "        merged.advertiser_id,\n",
    "        events.platform,\n",
    "        events.display_id,\n",
    "        events.uuid\n",
    "    from \n",
    "        merged\n",
    "    join \n",
    "        events \n",
    "    on \n",
    "        merged.document_id = events.document_id\n",
    "''')\n",
    "merged.registerTempTable(\"merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+-----------+-------------+--------+----------+--------------+\n",
      "|clicked|ad_id|document_id|campaign_id|advertiser_id|platform|display_id|          uuid|\n",
      "+-------+-----+-----------+-----------+-------------+--------+----------+--------------+\n",
      "|      0|85515|    1084599|       1633|          617|       1|    682390|7592dfa44f29b6|\n",
      "|      0|85515|    1084599|       1633|          617|       1|  16985153|c6389efa0eedc1|\n",
      "|      0|85515|    1084599|       1633|          617|       1|  18648077|2c4e6c8e09ff70|\n",
      "|      0|85515|    1084599|       1633|          617|       1|   8823719|6df7e3724b72c5|\n",
      "|      0|85515|    1084599|       1633|          617|       3|  14515033|89b6e9b40a406d|\n",
      "+-------+-----+-----------+-----------+-------------+--------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|          uuid|geo_location|\n",
      "+--------------+------------+\n",
      "|cb8c55702adb93|   US>SC>519|\n",
      "|79a85fa78311b9|   US>CA>807|\n",
      "|822932ce3d8757|   US>MI>505|\n",
      "|85281d0a49f7ac|   US>WV>564|\n",
      "|8d0daef4bf5b56|       SG>00|\n",
      "+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_df = ss.sql(\"select uuid, geo_location from events\")\n",
    "geo_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "geo_df = ss.createDataFrame(geo_df.rdd.map(lambda x: Row(\n",
    "    geo_location=x.geo_location.split(\">\") + ['', ''] if x.geo_location else ['', '', ''],\n",
    "    uuid=x.uuid)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+\n",
      "|     geo_location|          uuid|\n",
      "+-----------------+--------------+\n",
      "|[US, SC, 519, , ]|cb8c55702adb93|\n",
      "|[US, CA, 807, , ]|79a85fa78311b9|\n",
      "|[US, MI, 505, , ]|822932ce3d8757|\n",
      "|[US, WV, 564, , ]|85281d0a49f7ac|\n",
      "|     [SG, 00, , ]|8d0daef4bf5b56|\n",
      "|[US, OH, 510, , ]|7765b4faae4ad4|\n",
      "|[US, MT, 762, , ]|2cc3f6457d16da|\n",
      "|[US, PA, 566, , ]|166fc654d73c98|\n",
      "|[US, FL, 528, , ]|9dddccf70f6067|\n",
      "|         [US, , ]|b09a0e92aa4d17|\n",
      "+-----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_df = geo_df.rdd.map(lambda x: Row(\n",
    "    country=x.geo_location[0],\n",
    "    state=x.geo_location[1],\n",
    "    dma=x.geo_location[2],\n",
    "    uuid=x.uuid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = ss.createDataFrame(geo_df)\n",
    "df.registerTempTable(\"geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+--------------+\n",
      "|country|dma|state|          uuid|\n",
      "+-------+---+-----+--------------+\n",
      "|     US|519|   SC|cb8c55702adb93|\n",
      "|     US|807|   CA|79a85fa78311b9|\n",
      "|     US|505|   MI|822932ce3d8757|\n",
      "|     US|564|   WV|85281d0a49f7ac|\n",
      "|     SG|   |   00|8d0daef4bf5b56|\n",
      "|     US|510|   OH|7765b4faae4ad4|\n",
      "|     US|762|   MT|2cc3f6457d16da|\n",
      "|     US|566|   PA|166fc654d73c98|\n",
      "|     US|528|   FL|9dddccf70f6067|\n",
      "|     US|   |     |b09a0e92aa4d17|\n",
      "+-------+---+-----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+-----------+-------------+--------+----------+--------------+\n",
      "|clicked|ad_id|document_id|campaign_id|advertiser_id|platform|display_id|          uuid|\n",
      "+-------+-----+-----------+-----------+-------------+--------+----------+--------------+\n",
      "|      0|85515|    1084599|       1633|          617|       1|    682390|7592dfa44f29b6|\n",
      "|      0|85515|    1084599|       1633|          617|       1|  16985153|c6389efa0eedc1|\n",
      "|      0|85515|    1084599|       1633|          617|       1|   3438587|bba6c1624943ed|\n",
      "|      0|85515|    1084599|       1633|          617|       1|  12669272|7b3fa7dbbd2869|\n",
      "|      0|85515|    1084599|       1633|          617|       1|   8823719|6df7e3724b72c5|\n",
      "+-------+-----+-----------+-----------+-------------+--------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_name=u'clicked', data_type=u'string', comment=None),\n",
       " Row(col_name=u'ad_id', data_type=u'string', comment=None),\n",
       " Row(col_name=u'document_id', data_type=u'string', comment=None),\n",
       " Row(col_name=u'campaign_id', data_type=u'string', comment=None),\n",
       " Row(col_name=u'advertiser_id', data_type=u'string', comment=None),\n",
       " Row(col_name=u'platform', data_type=u'string', comment=None),\n",
       " Row(col_name=u'display_id', data_type=u'string', comment=None),\n",
       " Row(col_name=u'uuid', data_type=u'string', comment=None)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql('''\n",
    "DESCRIBE merged\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_name=u'country', data_type=u'string', comment=None),\n",
       " Row(col_name=u'dma', data_type=u'string', comment=None),\n",
       " Row(col_name=u'state', data_type=u'string', comment=None),\n",
       " Row(col_name=u'uuid', data_type=u'string', comment=None)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql('''\n",
    "DESCRIBE geo\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged2 = ss.sql('''\n",
    "    select \n",
    "        merged.clicked,\n",
    "        merged.ad_id,\n",
    "        merged.document_id, \n",
    "        merged.campaign_id,\n",
    "        merged.advertiser_id,\n",
    "        merged.platform,\n",
    "        merged.display_id,\n",
    "        geo.country,\n",
    "        geo.dma,\n",
    "        geo.state\n",
    "    from \n",
    "        merged\n",
    "    join \n",
    "        geo \n",
    "    on \n",
    "        merged.uuid = geo.uuid\n",
    "''')\n",
    "merged2.registerTempTable(\"merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-242710463540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerged2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2.1/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2.1/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/hdp/current/spark2.1/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2.1/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Calculate features for VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Use DataFrame API to join tables (functions in SQL queries: https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html)\n",
    "- Use Python API to calculate features and save them as text for VW (*saveAsTextFile()*)\n",
    "- Hash features in Spark (24 bits, use *sklearn.utils.murmurhash.murmurhash3_32*)\n",
    "- Split dataset in Spark into 90% train, 10% test **by display_id**, save the split for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_32\n",
    "def hasher(x, bits):\n",
    "    return murmurhash3_32(x) % 2**bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Copy data from HDFS to cluster1 machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will run vowpal wabbit **locally**, need to copy data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_text_to_local(hdfs_path, local_path):\n",
    "    if os.path.exists(local_path):\n",
    "        shutil.rmtree(local_path)\n",
    "    os.mkdir(local_path)\n",
    "    os.system('hadoop fs -copyToLocal \"{0}/*\" {1}'.format(hdfs_path, local_path))\n",
    "    os.system('cat {0}/part-* > {1}'.format(local_path, local_path + \"/merged.txt\"))\n",
    "    print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/train.txt\", \"/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 0 May 13 12:19 /data/train.txt/merged.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /data/train.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_text_to_local(\"/task1/test.txt\", \"/data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 0 May 13 12:19 /data/test.txt/merged.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /data/test.txt/merged.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Install VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  python-chardet-whl python-colorama python-colorama-whl python-distlib\n",
      "  python-distlib-whl python-html5lib python-html5lib-whl python-pip-whl\n",
      "  python-requests-whl python-setuptools-whl python-six-whl python-urllib3-whl\n",
      "  python-wheel python3-pkg-resources\n",
      "Use 'apt-get autoremove' to remove them.\n",
      "The following extra packages will be installed:\n",
      "  autoconf autotools-dev libboost-program-options1.54-dev\n",
      "  libboost-program-options1.54.0 libboost-python1.54-dev libboost-python1.54.0\n",
      "  libboost1.54-dev libltdl-dev libltdl7\n",
      "Suggested packages:\n",
      "  autoconf2.13 autoconf-archive gnu-standards autoconf-doc gettext\n",
      "  libboost1.54-doc python-pyste libboost-atomic1.54-dev\n",
      "  libboost-chrono1.54-dev libboost-context1.54-dev libboost-coroutine.54-dev\n",
      "  libboost-date-time1.54-dev libboost-exception1.54-dev\n",
      "  libboost-filesystem1.54-dev libboost-graph1.54-dev\n",
      "  libboost-graph-parallel1.54-dev libboost-iostreams1.54-dev\n",
      "  libboost-locale1.54-dev libboost-log.54-dev libboost-math1.54-dev\n",
      "  libboost-mpi1.54-dev libboost-mpi-python1.54-dev libboost-random1.54-dev\n",
      "  libboost-regex1.54-dev libboost-serialization1.54-dev\n",
      "  libboost-signals1.54-dev libboost-system1.54-dev libboost-test1.54-dev\n",
      "  libboost-thread1.54-dev libboost-timer1.54-dev libboost-wave1.54-dev\n",
      "  libboost1.54-tools-dev libmpfrc++-dev libntl-dev libtool-doc automaken\n",
      "  gfortran fortran95-compiler gcj-jdk\n",
      "The following NEW packages will be installed:\n",
      "  autoconf automake autotools-dev libboost-program-options-dev\n",
      "  libboost-program-options1.54-dev libboost-program-options1.54.0\n",
      "  libboost-python-dev libboost-python1.54-dev libboost-python1.54.0\n",
      "  libboost1.54-dev libltdl-dev libltdl7 libtool m4 zlib1g-dev\n",
      "0 upgraded, 15 newly installed, 0 to remove and 132 not upgraded.\n",
      "Need to get 7812 kB of archives.\n",
      "After this operation, 107 MB of additional disk space will be used.\n",
      "Get:1 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-program-options1.54.0 amd64 1.54.0-4ubuntu3.1 [115 kB]\n",
      "Get:2 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-python1.54.0 amd64 1.54.0-4ubuntu3.1 [122 kB]\n",
      "Get:3 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libltdl7 amd64 2.4.2-1.7ubuntu1 [35.0 kB]\n",
      "Get:4 http://azure.archive.ubuntu.com/ubuntu/ trusty/main m4 amd64 1.4.17-2ubuntu1 [195 kB]\n",
      "Get:5 http://azure.archive.ubuntu.com/ubuntu/ trusty/main autoconf all 2.69-6 [322 kB]\n",
      "Get:6 http://azure.archive.ubuntu.com/ubuntu/ trusty/main autotools-dev all 20130810.1 [44.3 kB]\n",
      "Get:7 http://azure.archive.ubuntu.com/ubuntu/ trusty/main automake all 1:1.14.1-2ubuntu1 [510 kB]\n",
      "Get:8 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost1.54-dev amd64 1.54.0-4ubuntu3.1 [5682 kB]\n",
      "Get:9 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-program-options1.54-dev amd64 1.54.0-4ubuntu3.1 [133 kB]\n",
      "Get:10 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libboost-program-options-dev amd64 1.54.0.1ubuntu1 [2840 B]\n",
      "Get:11 http://azure.archive.ubuntu.com/ubuntu/ trusty-updates/main libboost-python1.54-dev amd64 1.54.0-4ubuntu3.1 [119 kB]\n",
      "Get:12 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libboost-python-dev amd64 1.54.0.1ubuntu1 [3204 B]\n",
      "Get:13 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libltdl-dev amd64 2.4.2-1.7ubuntu1 [157 kB]\n",
      "Get:14 http://azure.archive.ubuntu.com/ubuntu/ trusty/main libtool amd64 2.4.2-1.7ubuntu1 [188 kB]\n",
      "Get:15 http://azure.archive.ubuntu.com/ubuntu/ trusty/main zlib1g-dev amd64 1:1.2.8.dfsg-1ubuntu1 [183 kB]\n",
      "Fetched 7812 kB in 0s (18.6 MB/s)     \n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n",
      "locale: Cannot set LC_CTYPE to default locale: No such file or directory\n",
      "locale: Cannot set LC_ALL to default locale: No such file or directory\n",
      "Selecting previously unselected package libboost-program-options1.54.0:amd64.\n",
      "(Reading database ... 56298 files and directories currently installed.)\n",
      "Preparing to unpack .../libboost-program-options1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-python1.54.0:amd64.\n",
      "Preparing to unpack .../libboost-python1.54.0_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-python1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libltdl7:amd64.\n",
      "Preparing to unpack .../libltdl7_2.4.2-1.7ubuntu1_amd64.deb ...\n",
      "Unpacking libltdl7:amd64 (2.4.2-1.7ubuntu1) ...\n",
      "Selecting previously unselected package m4.\n",
      "Preparing to unpack .../m4_1.4.17-2ubuntu1_amd64.deb ...\n",
      "Unpacking m4 (1.4.17-2ubuntu1) ...\n",
      "Selecting previously unselected package autoconf.\n",
      "Preparing to unpack .../autoconf_2.69-6_all.deb ...\n",
      "Unpacking autoconf (2.69-6) ...\n",
      "Selecting previously unselected package autotools-dev.\n",
      "Preparing to unpack .../autotools-dev_20130810.1_all.deb ...\n",
      "Unpacking autotools-dev (20130810.1) ...\n",
      "Selecting previously unselected package automake.\n",
      "Preparing to unpack .../automake_1%3a1.14.1-2ubuntu1_all.deb ...\n",
      "Unpacking automake (1:1.14.1-2ubuntu1) ...\n",
      "Selecting previously unselected package libboost1.54-dev.\n",
      "Preparing to unpack .../libboost1.54-dev_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost1.54-dev (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-program-options1.54-dev:amd64.\n",
      "Preparing to unpack .../libboost-program-options1.54-dev_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-program-options1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-program-options-dev:amd64.\n",
      "Preparing to unpack .../libboost-program-options-dev_1.54.0.1ubuntu1_amd64.deb ...\n",
      "Unpacking libboost-program-options-dev:amd64 (1.54.0.1ubuntu1) ...\n",
      "Selecting previously unselected package libboost-python1.54-dev:amd64.\n",
      "Preparing to unpack .../libboost-python1.54-dev_1.54.0-4ubuntu3.1_amd64.deb ...\n",
      "Unpacking libboost-python1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Selecting previously unselected package libboost-python-dev.\n",
      "Preparing to unpack .../libboost-python-dev_1.54.0.1ubuntu1_amd64.deb ...\n",
      "Unpacking libboost-python-dev (1.54.0.1ubuntu1) ...\n",
      "Selecting previously unselected package libltdl-dev:amd64.\n",
      "Preparing to unpack .../libltdl-dev_2.4.2-1.7ubuntu1_amd64.deb ...\n",
      "Unpacking libltdl-dev:amd64 (2.4.2-1.7ubuntu1) ...\n",
      "Selecting previously unselected package libtool.\n",
      "Preparing to unpack .../libtool_2.4.2-1.7ubuntu1_amd64.deb ...\n",
      "Unpacking libtool (2.4.2-1.7ubuntu1) ...\n",
      "Selecting previously unselected package zlib1g-dev:amd64.\n",
      "Preparing to unpack .../zlib1g-dev_1%3a1.2.8.dfsg-1ubuntu1_amd64.deb ...\n",
      "Unpacking zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...\n",
      "Processing triggers for man-db (2.6.7.1-1ubuntu1) ...\n",
      "Processing triggers for install-info (5.2.0.dfsg.1-2) ...\n",
      "Setting up libboost-program-options1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-python1.54.0:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libltdl7:amd64 (2.4.2-1.7ubuntu1) ...\n",
      "Setting up m4 (1.4.17-2ubuntu1) ...\n",
      "Setting up autoconf (2.69-6) ...\n",
      "Setting up autotools-dev (20130810.1) ...\n",
      "Setting up automake (1:1.14.1-2ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/automake-1.14 to provide /usr/bin/automake (automake) in auto mode\n",
      "Setting up libboost1.54-dev (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-program-options1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-program-options-dev:amd64 (1.54.0.1ubuntu1) ...\n",
      "Setting up libboost-python1.54-dev:amd64 (1.54.0-4ubuntu3.1) ...\n",
      "Setting up libboost-python-dev (1.54.0.1ubuntu1) ...\n",
      "Setting up libltdl-dev:amd64 (2.4.2-1.7ubuntu1) ...\n",
      "Setting up libtool (2.4.2-1.7ubuntu1) ...\n",
      "Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.19-0ubuntu6.9) ...\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get install libboost-program-options-dev zlib1g-dev libboost-python-dev libtool m4 automake -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-13 12:19:50--  https://github.com/JohnLangford/vowpal_wabbit/archive/8.2.0.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/JohnLangford/vowpal_wabbit/tar.gz/8.2.0 [following]\n",
      "--2017-05-13 12:19:51--  https://codeload.github.com/JohnLangford/vowpal_wabbit/tar.gz/8.2.0\n",
      "Resolving codeload.github.com (codeload.github.com)... 192.30.253.120, 192.30.253.121\n",
      "Connecting to codeload.github.com (codeload.github.com)|192.30.253.120|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: '8.2.0.tar.gz'\n",
      "\n",
      "    [     <=>                               ] 13,391,191  11.9MB/s   in 1.1s   \n",
      "\n",
      "2017-05-13 12:19:52 (11.9 MB/s) - '8.2.0.tar.gz' saved [13391191]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/JohnLangford/vowpal_wabbit/archive/8.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! tar -xzf 8.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "cd vowpal_wabbit-8.2.0\n",
    "\n",
    "./autogen.sh\n",
    "\n",
    "make -j4\n",
    "\n",
    "make test -j4\n",
    "\n",
    "sudo make install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! head -n2 /data/train.txt/merged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: vw: not found\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! LD_LIBRARY_PATH=/usr/local/lib vw -d /data/train.txt/merged.txt -b 24 -c -k --ftrl --passes 1 -f /data/model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Check VW test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: vw: not found\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! LD_LIBRARY_PATH=/usr/local/lib vw -d /data/test.txt/merged.txt -i /data/model -t -k -p /data/test_predictions.txt --progress 1000000 --link=logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/data/test_predictions.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-6c3d164a1910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_vw_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/test_predictions.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-6c3d164a1910>\u001b[0m in \u001b[0;36mread_vw_predictions\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_vw_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/data/test_predictions.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_vw_predictions(p):\n",
    "    y_pred = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_pred.append(float(line.split()[0]))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "y_pred = read_vw_predictions(\"/data/test_predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_vw_y_true(p):\n",
    "    y_true = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_true.append(float(line.partition(\" \")[0]))\n",
    "    return np.array(y_true)\n",
    "\n",
    "y_true = get_vw_y_true(\"/data/test.txt/merged.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Make submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This time let's make a personalized recommender using:\n",
    "- page views information\n",
    "- document properties\n",
    "\n",
    "Ideas for features:\n",
    "- uuid topic, entity, publisher, ... preferences\n",
    "- document similarities\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More SQL examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we start with a DataFrame\n",
    "events_df = ss.sql(\"select * from events\")\n",
    "events_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we can make RDD of Rows with *.rdd\n",
    "from pyspark.sql import Row\n",
    "events_df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# When it's RDD, we can use Python to create new RDD of Rows\n",
    "(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n",
    ").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we can convert it back to DataFrame if it's still a table that can be converted to Java types\n",
    "ss.createDataFrame(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# we can save it to HDFS as parquet (if it's a DataFrame)\n",
    "ss.createDataFrame(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n",
    ").write.mode(\"overwrite\").parquet(\"/task1/example1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/example1\").printSchema()\n",
    "ss.read.parquet(\"/task1/example1\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# or we can skip DataFrame API if we use Python functions (there will be no speed increase)\n",
    "(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n",
    ").saveAsPickleFile(\"/task1/example2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.pickleFile(\"/task1/example2\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sometimes we cannot make a DataFrame\n",
    "import numpy as np\n",
    "rdd = (\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(x=np.array(x.geo_location.split(\">\") if x.geo_location else [])))\n",
    ")\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# throws TypeError: not supported type: <type 'numpy.ndarray'>\n",
    "ss.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# but we can save as RDD in pickle file just fine\n",
    "rdd.saveAsPickleFile(\"/task1/example3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Takeaways:\n",
    "- use DataFrames when you can (simple join's, select's, groupby's), it will be faster\n",
    "- use RDD and Python when you can't use DataFrame API\n",
    "- convert it back to DataFrame if needed\n",
    "- or save to pickles (can save almost any Python object as pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Built-in SQL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can find more at https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sql version\n",
    "df = ss.sql(\"\"\"\n",
    "select\n",
    "    document_id,\n",
    "    collect_list(struct(category_id, confidence_level)) as categories\n",
    "from\n",
    "    documents_categories\n",
    "group by document_id\n",
    "\"\"\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.write.mode(\"overwrite\").parquet(\"/task1/example4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# or we can use RDD and Python if we are not aware of those SQL functions\n",
    "rdd = (\n",
    "    ss.sql(\"select * from documents_categories\")\n",
    "    .rdd\n",
    "    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n",
    "    .groupByKey()\n",
    "    .map(lambda (k, vs): (k, list(vs)))\n",
    ")\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# it's much slower, but we can do almost everything in Python\n",
    "rdd.saveAsPickleFile(\"/task1/example5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# but sometimes with Python we can do more\n",
    "rdd = (\n",
    "    ss.sql(\"select * from documents_categories\")\n",
    "    .rdd\n",
    "    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n",
    "    .groupByKey()\n",
    "    .map(lambda (k, vs): Row(document_id=k, categories={a: float(b) for a, b in vs}))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "ss.createDataFrame(rdd).write.parquet(\"/task1/example6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.read.parquet(\"/task1/example6\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# now we can join this table with events for instance\n",
    "ss.read.parquet(\"/task1/example6\").registerTempTable(\"doc_categories_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ss.sql(\"\"\"\n",
    "select \n",
    "    e.*, \n",
    "    dc.categories\n",
    "from \n",
    "    events as e\n",
    "    join doc_categories_ready as dc on dc.document_id = e.document_id\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

